{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829e5d3-e36a-494f-a097-d8be722793af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# 设置随机种子保证可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设备配置（自动检测GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== 需要修改的部分 ====================\n",
    "# 1. 数据路径配置 - 修改为你的新数据路径\n",
    "new_data_dir = os.getcwd()  # 修改：新的原图数据目录\n",
    "new_image_dir = os.path.join(new_data_dir, \"Ftest\")  # 修改：新图像目录\n",
    "new_annotation_file = os.path.join(new_data_dir, \"Ftest.xlsx\")  # 修改：新标注文件\n",
    "\n",
    "# 2. 模型路径配置 - 修改为你训练好的模型路径\n",
    "trained_model_path = os.path.join(new_data_dir, \"F.pth\")  # 修改：选择训练好的模型文件\n",
    "\n",
    "# 3. 结果保存路径\n",
    "results_save_dir = os.path.join(new_data_dir, \"Ftestresults\")  # 修改：验证结果保存目录\n",
    "os.makedirs(results_save_dir, exist_ok=True)\n",
    "# =====================================================\n",
    "\n",
    "# 加载模型定义（需要与训练时相同的模型类）\n",
    "class SoftMaskConstrainedHeatmapLayer(nn.Module):\n",
    "    \"\"\"宽松约束的热图生成层\"\"\"\n",
    "    def __init__(self, in_channels, num_classes, heatmap_size=(56, 56)):\n",
    "        super(SoftMaskConstrainedHeatmapLayer, self).__init__()\n",
    "        self.heatmap_size = heatmap_size\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "    \n",
    "    def forward(self, features, binary_mask=None):\n",
    "        raw_heatmap = self.conv(features)\n",
    "        \n",
    "        if binary_mask is not None:\n",
    "            if binary_mask.size()[-2:] != raw_heatmap.size()[-2:]:\n",
    "                binary_mask = F.interpolate(binary_mask, size=raw_heatmap.size()[-2:], \n",
    "                                          mode='bilinear', align_corners=False)\n",
    "            soft_mask = binary_mask * 0.8 + 0.2\n",
    "            constrained_heatmap = raw_heatmap * soft_mask\n",
    "        else:\n",
    "            constrained_heatmap = raw_heatmap\n",
    "        \n",
    "        return constrained_heatmap\n",
    "\n",
    "class RenalFibrosisModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, model_name='resnet18', pretrained=True, mask_constraint='soft'):\n",
    "        super(RenalFibrosisModel, self).__init__()\n",
    "        self.mask_constraint = mask_constraint\n",
    "        \n",
    "        if model_name == 'resnet18':\n",
    "            self.backbone = models.resnet18(pretrained=pretrained)\n",
    "            self.features = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "            num_features = self.backbone.fc.in_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        self.heatmap_layer = SoftMaskConstrainedHeatmapLayer(num_features, num_classes)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        features = self.features(x)\n",
    "        \n",
    "        mask_resized = None\n",
    "        if mask is not None:\n",
    "            mask_resized = F.interpolate(mask, size=features.size()[-2:], \n",
    "                                       mode='bilinear', align_corners=False)\n",
    "        \n",
    "        heatmap = self.heatmap_layer(features, mask_resized)\n",
    "        pooled = self.global_avg_pool(features)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        outputs = self.classifier(pooled)\n",
    "        \n",
    "        return outputs, heatmap, features\n",
    "\n",
    "def load_new_annotations(annotation_file, image_dir):\n",
    "    \"\"\"加载新的标注数据（无掩码）\"\"\"\n",
    "    try:\n",
    "        annotations = pd.read_excel(annotation_file)\n",
    "        \n",
    "        # 自动检测标签列\n",
    "        label_col = None\n",
    "        possible_label_names = ['LABLE', 'LABEL', 'Label', 'label', '分级', '评分', 'fibrosis', 'stage']\n",
    "        for name in possible_label_names:\n",
    "            if name in annotations.columns:\n",
    "                label_col = name\n",
    "                break\n",
    "        if label_col is None:\n",
    "            raise ValueError(\"无法找到标签列\")\n",
    "        \n",
    "        annotations.rename(columns={label_col: 'LABEL'}, inplace=True)\n",
    "        annotations['ID'] = annotations['ID'].astype(str).str.strip()\n",
    "        \n",
    "        # 处理标签 - 转换为0/1二分类\n",
    "        unique_labels = sorted(annotations['LABEL'].unique())\n",
    "        if len(unique_labels) != 2:\n",
    "            if len(unique_labels) > 2:\n",
    "                print(f\"警告: 检测到多类标签{unique_labels}, 将转换为二分类问题\")\n",
    "                annotations['LABEL'] = (annotations['LABEL'] >= 3).astype(int)\n",
    "            else:\n",
    "                raise ValueError(\"标签类别不足2类\")\n",
    "        \n",
    "        # 添加图像路径列\n",
    "        def find_image_path(x):\n",
    "            base_path = os.path.join(image_dir, x)\n",
    "            for ext in ['.jpg', '.tif', '.tiff', '.png', '.jpeg']:\n",
    "                img_path = base_path + ext\n",
    "                if os.path.exists(img_path):\n",
    "                    return img_path\n",
    "                img_path = base_path + ext.upper()\n",
    "                if os.path.exists(img_path):\n",
    "                    return img_path\n",
    "            return None\n",
    "        \n",
    "        annotations['image_path'] = annotations['ID'].apply(find_image_path)\n",
    "        \n",
    "        # 检查是否有图像缺失\n",
    "        missing_images = annotations['image_path'].isnull().sum()\n",
    "        if missing_images > 0:\n",
    "            print(f\"警告: 有{missing_images}个ID找不到对应的图像文件\")\n",
    "        \n",
    "        # 只保留有图像的有效样本\n",
    "        annotations = annotations.dropna(subset=['image_path'])\n",
    "        \n",
    "        if len(annotations) == 0:\n",
    "            raise ValueError(\"没有找到有效的样本\")\n",
    "        \n",
    "        print(f\"找到{len(annotations)}个有效样本\")\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"加载标注文件出错: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class NewDataDataset(Dataset):\n",
    "    \"\"\"新的数据集类（无掩码）\"\"\"\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe.copy()\n",
    "        self.transform = transform\n",
    "        self.valid_samples = []\n",
    "        \n",
    "        # 预先检查所有图像可用性\n",
    "        for idx, row in self.dataframe.iterrows():\n",
    "            img_path = row['image_path']\n",
    "            \n",
    "            if isinstance(img_path, str) and os.path.exists(img_path):\n",
    "                try:\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.verify()\n",
    "                    self.valid_samples.append((img_path, row['LABEL']))\n",
    "                except Exception as e:\n",
    "                    print(f\"警告: 文件损坏 {img_path}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"警告: 文件不存在 {img_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.valid_samples[idx]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, torch.tensor(label, dtype=torch.long), img_path\n",
    "        except Exception as e:\n",
    "            print(f\"加载文件 {img_path} 出错: {str(e)}\")\n",
    "            return torch.zeros(3, 224, 224), torch.tensor(-1, dtype=torch.long), img_path\n",
    "\n",
    "def get_validation_transforms():\n",
    "    \"\"\"获取验证用的数据变换\"\"\"\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return val_transforms\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_probs),\n",
    "        'pr_auc': average_precision_score(y_true, y_probs),\n",
    "        'kappa': cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def validate_model():\n",
    "    \"\"\"验证模型在主函数\"\"\"\n",
    "    \n",
    "    print(\"开始验证模型...\")\n",
    "    \n",
    "    # 加载新的标注数据\n",
    "    print(\"加载新的标注数据...\")\n",
    "    new_annotations = load_new_annotations(new_annotation_file, new_image_dir)\n",
    "    \n",
    "    # 获取数据变换\n",
    "    val_transforms = get_validation_transforms()\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    print(\"创建数据集...\")\n",
    "    new_dataset = NewDataDataset(new_annotations, transform=val_transforms)\n",
    "    \n",
    "    new_loader = DataLoader(\n",
    "        new_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    print(\"初始化模型...\")\n",
    "    model = RenalFibrosisModel(\n",
    "        num_classes=2,\n",
    "        model_name='resnet18',\n",
    "        pretrained=False,\n",
    "        mask_constraint='soft'\n",
    "    ).to(device)\n",
    "    \n",
    "    # 加载训练好的模型权重\n",
    "    print(f\"加载训练好的模型: {trained_model_path}\")\n",
    "    if not os.path.exists(trained_model_path):\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {trained_model_path}\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(trained_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"模型加载完成!\")\n",
    "    \n",
    "    # 进行预测\n",
    "    print(\"开始预测...\")\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_image_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels, img_paths) in enumerate(tqdm(new_loader, desc='Validation')):\n",
    "            # 过滤无效样本\n",
    "            valid_mask = labels != -1\n",
    "            if not valid_mask.any():\n",
    "                continue\n",
    "                \n",
    "            inputs = inputs[valid_mask].to(device)\n",
    "            labels = labels[valid_mask].to(device)\n",
    "            valid_img_paths = [img_paths[i] for i in range(len(valid_mask)) if valid_mask[i]]\n",
    "            \n",
    "            # 前向传播 - mask=None\n",
    "            outputs, _, _ = model(inputs, mask=None)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            # 收集结果\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_image_paths.extend(valid_img_paths)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    print(\"计算评估指标...\")\n",
    "    if len(all_labels) > 0:\n",
    "        metrics = calculate_metrics(\n",
    "            np.array(all_labels),\n",
    "            np.array(all_preds),\n",
    "            np.array(all_probs)\n",
    "        )\n",
    "        \n",
    "        # 打印详细结果\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"验证结果汇总\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"总样本数: {len(all_labels)}\")\n",
    "        print(f\"准确率: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"精确率: {metrics['precision']:.4f}\")\n",
    "        print(f\"召回率: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1分数: {metrics['f1']:.4f}\")\n",
    "        print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"PR-AUC: {metrics['pr_auc']:.4f}\")\n",
    "        print(f\"Kappa: {metrics['kappa']:.4f}\")\n",
    "        \n",
    "        # 分类报告\n",
    "        print(\"\\n详细分类报告:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))\n",
    "        \n",
    "        # 混淆矩阵\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Pred 0', 'Pred 1'], \n",
    "                   yticklabels=['True 0', 'True 1'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        cm_path = os.path.join(results_save_dir, 'confusion_matrix.png')\n",
    "        plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"混淆矩阵已保存: {cm_path}\")\n",
    "        \n",
    "        # ROC曲线\n",
    "        fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "        roc_auc = metrics['roc_auc']\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        roc_path = os.path.join(results_save_dir, 'roc_curve.png')\n",
    "        plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"ROC曲线已保存: {roc_path}\")\n",
    "        \n",
    "        # 保存详细结果\n",
    "        results = {\n",
    "            'model_path': trained_model_path,\n",
    "            'total_samples': len(all_labels),\n",
    "            'metrics': metrics,\n",
    "            'predictions': [\n",
    "                {\n",
    "                    'image_path': all_image_paths[i],\n",
    "                    'true_label': int(all_labels[i]),\n",
    "                    'predicted_label': int(all_preds[i]),\n",
    "                    'probability': float(all_probs[i])\n",
    "                }\n",
    "                for i in range(len(all_labels))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # 保存为JSON\n",
    "        results_path = os.path.join(results_save_dir, 'validation_results.json')\n",
    "        with open(results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        # 保存为CSV\n",
    "        results_df = pd.DataFrame({\n",
    "            'image_path': all_image_paths,\n",
    "            'true_label': all_labels,\n",
    "            'predicted_label': all_preds,\n",
    "            'probability': all_probs\n",
    "        })\n",
    "        csv_path = os.path.join(results_save_dir, 'validation_results.csv')\n",
    "        results_df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\n所有结果已保存到: {results_save_dir}\")\n",
    "        print(f\"- 验证结果(JSON): {results_path}\")\n",
    "        print(f\"- 验证结果(CSV): {csv_path}\")\n",
    "        print(f\"- 混淆矩阵: {cm_path}\")\n",
    "        print(f\"- ROC曲线: {roc_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"没有有效的预测结果!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTHON 3.11",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
