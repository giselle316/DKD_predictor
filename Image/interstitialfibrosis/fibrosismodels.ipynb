{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4309520-3573-4c71-84dd-fd526f823667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# 设置随机种子保证可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设备配置（自动检测GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 数据路径配置\n",
    "data_dir = os.getcwd()\n",
    "image_dir = os.path.join(data_dir, \"Fibrosis\")\n",
    "annotation_file = os.path.join(data_dir, \"Fibrosislabel.xlsx\")\n",
    "mask_dir = os.path.join(data_dir, \"Mask\")  \n",
    "\n",
    "# 新增：模型保存路径配置\n",
    "model_save_dir = os.path.join(data_dir, \"Model\")  # 修改为您想要保存模型的路径\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # 确保目录存在\n",
    "print(f\"模型将保存到: {model_save_dir}\")\n",
    "\n",
    "class SoftMaskConstrainedHeatmapLayer(nn.Module):\n",
    "    \"\"\"宽松约束的热图生成层，允许背景区域有一定信号\"\"\"\n",
    "    def __init__(self, in_channels, num_classes, heatmap_size=(56, 56)):\n",
    "        super(SoftMaskConstrainedHeatmapLayer, self).__init__()\n",
    "        self.heatmap_size = heatmap_size\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "        # 初始化权重\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "    \n",
    "    def forward(self, features, binary_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: 特征图 [B, C, H, W]\n",
    "            binary_mask: 二值掩码 [B, 1, H, W]，与特征图相同尺寸\n",
    "        Returns:\n",
    "            constrained_heatmap: 宽松约束后的热图 [B, num_classes, H, W]\n",
    "        \"\"\"\n",
    "        # 生成原始热图\n",
    "        raw_heatmap = self.conv(features)  # [B, num_classes, H, W]\n",
    "        \n",
    "        # 如果有掩码，应用宽松约束\n",
    "        if binary_mask is not None:\n",
    "            # 使用双线性插值保持平滑过渡\n",
    "            if binary_mask.size()[-2:] != raw_heatmap.size()[-2:]:\n",
    "                binary_mask = F.interpolate(binary_mask, size=raw_heatmap.size()[-2:], \n",
    "                                          mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # 宽松约束：掩码区域保留，背景区域减弱但不归零\n",
    "            # 方法1：添加基础值，背景保留20%信号\n",
    "            soft_mask = binary_mask * 0.8 + 0.2\n",
    "            \n",
    "            constrained_heatmap = raw_heatmap * soft_mask\n",
    "        else:\n",
    "            constrained_heatmap = raw_heatmap\n",
    "        \n",
    "        return constrained_heatmap\n",
    "\n",
    "class RenalFibrosisModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, model_name='resnet18', pretrained=True, mask_constraint='soft'):\n",
    "        super(RenalFibrosisModel, self).__init__()\n",
    "        self.mask_constraint = mask_constraint  # 'soft' 或 'strict'\n",
    "        \n",
    "        # 加载预训练模型\n",
    "        if model_name == 'resnet18':\n",
    "            self.backbone = models.resnet18(pretrained=pretrained)\n",
    "            # 移除最后的全连接层和平均池化层\n",
    "            self.features = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "            num_features = self.backbone.fc.in_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # 全局平均池化\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # 热图生成层（使用宽松约束版本）\n",
    "        self.heatmap_layer = SoftMaskConstrainedHeatmapLayer(num_features, num_classes)\n",
    "        \n",
    "        # 初始化分类器\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 输入图像 [B, 3, H, W]\n",
    "            mask: 二值掩码 [B, 1, H, W]\n",
    "        Returns:\n",
    "            outputs: 分类logits [B, num_classes]\n",
    "            heatmap: 宽松约束热图 [B, num_classes, H, W]\n",
    "            features: 特征图 [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # 提取特征\n",
    "        features = self.features(x)  # [B, 512, H/32, W/32]\n",
    "        \n",
    "        # 宽松约束：移除特征层的掩码约束，直接使用原始特征\n",
    "        # 只在热图生成阶段应用宽松约束\n",
    "        masked_features = features\n",
    "        \n",
    "        # 如果需要掩码用于热图生成，调整尺寸\n",
    "        mask_resized = None\n",
    "        if mask is not None:\n",
    "            mask_resized = F.interpolate(mask, size=features.size()[-2:], \n",
    "                                       mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 生成热图（应用宽松约束）\n",
    "        heatmap = self.heatmap_layer(masked_features, mask_resized)\n",
    "        \n",
    "        # 全局平均池化用于分类\n",
    "        pooled = self.global_avg_pool(masked_features)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        \n",
    "        # 分类输出\n",
    "        outputs = self.classifier(pooled)\n",
    "        \n",
    "        return outputs, heatmap, features\n",
    "    \n",
    "    def generate_attention_map(self, x, mask=None, class_idx=None):\n",
    "        \"\"\"生成宽松约束的热图\"\"\"\n",
    "        # 前向传播\n",
    "        outputs, heatmap, features = self.forward(x, mask)\n",
    "        \n",
    "        # 如果指定了类别，使用该类别的热图通道\n",
    "        if class_idx is not None:\n",
    "            attention_map = heatmap[:, class_idx:class_idx+1]  # [B, 1, H, W]\n",
    "        else:\n",
    "            # 使用所有类别的平均热图\n",
    "            attention_map = heatmap.mean(dim=1, keepdim=True)  # [B, 1, H, W]\n",
    "        \n",
    "        # 上采样到输入图像尺寸\n",
    "        attention_map = F.interpolate(attention_map, size=x.size()[-2:], \n",
    "                                    mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 宽松约束：不强制应用掩码，或应用轻微约束\n",
    "        if mask is not None and self.mask_constraint == 'soft':\n",
    "            # 轻微约束：背景区域保留30%信号\n",
    "            soft_final_mask = mask * 0.6 + 0.4\n",
    "            attention_map = attention_map * soft_final_mask\n",
    "        \n",
    "        # 归一化到[0, 1]\n",
    "        attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min() + 1e-8)\n",
    "        \n",
    "        return attention_map.squeeze(1)  # [B, H, W]\n",
    "\n",
    "def load_and_preprocess_annotations(annotation_file):\n",
    "    \"\"\"加载并预处理标注数据，现在包含掩码路径\"\"\"\n",
    "    try:\n",
    "        annotations = pd.read_excel(annotation_file)\n",
    "        \n",
    "        # 自动检测标签列\n",
    "        label_col = None\n",
    "        possible_label_names = ['LABLE', 'LABEL', 'Label', 'label', '分级', '评分', 'fibrosis', 'stage']\n",
    "        for name in possible_label_names:\n",
    "            if name in annotations.columns:\n",
    "                label_col = name\n",
    "                break\n",
    "        if label_col is None:\n",
    "            raise ValueError(\"无法找到标签列\")\n",
    "        \n",
    "        annotations.rename(columns={label_col: 'LABEL'}, inplace=True)\n",
    "        annotations['ID'] = annotations['ID'].astype(str).str.strip()\n",
    "        \n",
    "        # 处理标签 - 转换为0/1二分类\n",
    "        unique_labels = sorted(annotations['LABEL'].unique())\n",
    "        if len(unique_labels) != 2:\n",
    "            if len(unique_labels) > 2:\n",
    "                print(f\"警告: 检测到多类标签{unique_labels}, 将转换为二分类问题\")\n",
    "                annotations['LABEL'] = (annotations['LABEL'] >= 3).astype(int)\n",
    "            else:\n",
    "                raise ValueError(\"标签类别不足2类\")\n",
    "        \n",
    "        # 添加图像路径列\n",
    "        def find_image_path(x):\n",
    "            base_path = os.path.join(image_dir, x)\n",
    "            for ext in ['.jpg', '.tif', '.tiff', '.png', '.jpeg']:\n",
    "                img_path = base_path + ext\n",
    "                if os.path.exists(img_path):\n",
    "                    return img_path\n",
    "                img_path = base_path + ext.upper()\n",
    "                if os.path.exists(img_path):\n",
    "                    return img_path\n",
    "            return None\n",
    "        \n",
    "        annotations['image_path'] = annotations['ID'].apply(find_image_path)\n",
    "        \n",
    "        # 添加掩码路径列\n",
    "        def find_mask_path(x):\n",
    "            base_path = os.path.join(mask_dir, x)\n",
    "            for ext in ['.png', '.jpg', '.tif', '.tiff']:\n",
    "                mask_path = base_path + '_mask' + ext\n",
    "                if os.path.exists(mask_path):\n",
    "                    return mask_path\n",
    "                mask_path = base_path + '_mask' + ext.upper()\n",
    "                if os.path.exists(mask_path):\n",
    "                    return mask_path\n",
    "                # 也尝试直接使用ID作为文件名\n",
    "                mask_path = os.path.join(mask_dir, x + ext)\n",
    "                if os.path.exists(mask_path):\n",
    "                    return mask_path\n",
    "            return None\n",
    "        \n",
    "        annotations['mask_path'] = annotations['ID'].apply(find_mask_path)\n",
    "        \n",
    "        # 检查是否有图像或掩码缺失\n",
    "        missing_images = annotations['image_path'].isnull().sum()\n",
    "        missing_masks = annotations['mask_path'].isnull().sum()\n",
    "        \n",
    "        if missing_images > 0:\n",
    "            print(f\"\\n警告: 有{missing_images}个ID找不到对应的图像文件\")\n",
    "        if missing_masks > 0:\n",
    "            print(f\"\\n警告: 有{missing_masks}个ID找不到对应的掩码文件\")\n",
    "        \n",
    "        # 只保留既有图像又有掩码的样本\n",
    "        annotations = annotations.dropna(subset=['image_path', 'mask_path'])\n",
    "        \n",
    "        if len(annotations) == 0:\n",
    "            raise ValueError(\"没有找到同时包含图像和掩码的有效样本\")\n",
    "        \n",
    "        print(f\"\\n找到{len(annotations)}个有效样本（同时包含图像和掩码）\")\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"加载标注文件出错: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class RenalFibrosisDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.dataframe = dataframe.copy()\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # 与图像预处理一致\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.valid_samples = []\n",
    "        \n",
    "        # 预先检查所有图像和掩码可用性\n",
    "        for idx, row in self.dataframe.iterrows():\n",
    "            img_path = row['image_path']\n",
    "            mask_path = row['mask_path']\n",
    "            \n",
    "            if (isinstance(img_path, str) and os.path.exists(img_path) and\n",
    "                isinstance(mask_path, str) and os.path.exists(mask_path)):\n",
    "                try:\n",
    "                    # 验证图像\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.verify()\n",
    "                    \n",
    "                    # 验证掩码\n",
    "                    with Image.open(mask_path) as mask:\n",
    "                        mask.verify()\n",
    "                    \n",
    "                    self.valid_samples.append((img_path, mask_path, row['LABEL']))\n",
    "                except Exception as e:\n",
    "                    print(f\"警告: 文件损坏 {img_path} 或 {mask_path}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"警告: 文件不存在 {img_path} 或 {mask_path}\")\n",
    "        \n",
    "        # 计算类别权重\n",
    "        labels = [label for _, _, label in self.valid_samples]\n",
    "        class_counts = Counter(labels)\n",
    "        total = sum(class_counts.values())\n",
    "        self.class_weights = torch.tensor([total/class_counts[0], total/class_counts[1]] if len(class_counts) == 2 else [1.0, 1.0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, label = self.valid_samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # 加载图像\n",
    "            with Image.open(img_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            # 加载掩码\n",
    "            with Image.open(mask_path) as mask_img:\n",
    "                mask = mask_img.convert('L')  # 转为灰度图\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            if self.mask_transform:\n",
    "                mask = self.mask_transform(mask)\n",
    "                # 确保掩码是二值的\n",
    "                mask = (mask > 0.5).float()\n",
    "            \n",
    "            return image, mask, torch.tensor(label, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"加载文件 {img_path} 或 {mask_path} 出错: {str(e)}\")\n",
    "            # 返回空白数据\n",
    "            return torch.zeros(3, 224, 224), torch.zeros(1, 224, 224), torch.tensor(-1, dtype=torch.long)\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"获取数据增强变换\"\"\"\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=True, path='checkpoint.pth'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    \"\"\"计算所有评估指标\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_probs),\n",
    "        'pr_auc': average_precision_score(y_true, y_probs),\n",
    "        'kappa': cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def visualize_heatmap(image, mask, heatmap, save_path, mask_strength=0.6):\n",
    "    \"\"\"可视化热图，应用宽松的掩码约束\"\"\"\n",
    "    # 转换为numpy\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
    "    image_np = np.clip(image_np, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    mask_np = mask.squeeze().cpu().numpy() if mask.dim() == 3 else mask.cpu().numpy()\n",
    "    heatmap_np = heatmap.squeeze().cpu().numpy()\n",
    "    \n",
    "    # 宽松约束：背景区域保留部分信号\n",
    "    if mask_strength < 1.0:\n",
    "        soft_mask = mask_np * mask_strength + (1 - mask_strength)\n",
    "        heatmap_np = heatmap_np * soft_mask\n",
    "    \n",
    "    # 归一化热图\n",
    "    if heatmap_np.max() > heatmap_np.min():\n",
    "        heatmap_np = (heatmap_np - heatmap_np.min()) / (heatmap_np.max() - heatmap_np.min())\n",
    "    \n",
    "    # 创建热图彩色图\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_np), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 叠加到原图\n",
    "    overlayed = cv2.addWeighted(image_np, 0.6, heatmap_colored, 0.3, 0)\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(image_np)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask_np, cmap='gray')\n",
    "    axes[1].set_title('Binary Mask')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(heatmap_np, cmap='jet')\n",
    "    axes[2].set_title('Soft Attention Heatmap')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    axes[3].imshow(overlayed)\n",
    "    axes[3].set_title('Overlayed Result')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    def convert_tensors_to_serializable(obj):\n",
    "        if torch.is_tensor(obj):\n",
    "            return obj.item() if obj.numel() == 1 else obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_tensors_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_tensors_to_serializable(x) for x in obj]\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    # 加载数据\n",
    "    annotations = load_and_preprocess_annotations(annotation_file)\n",
    "    \n",
    "    # 超参数配置 - 使用宽松约束\n",
    "    config = {\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 40,\n",
    "        'learning_rate': 0.0001,\n",
    "        'weight_decay': 1e-4,\n",
    "        'model_name': 'resnet18',\n",
    "        'k_folds': 5,\n",
    "        'patience': 7,\n",
    "        'mask_constraint': 'soft',  # 改为宽松约束模式\n",
    "        'mask_strength': 0.6,      # 掩码约束强度（0-1）\n",
    "        'heatmap_lambda': 0.1,\n",
    "        'model_save_dir': model_save_dir  # 新增：模型保存路径\n",
    "    }\n",
    "    \n",
    "    # 获取数据增强\n",
    "    train_transforms, val_transforms = get_transforms()\n",
    "    \n",
    "    # 掩码变换（使用双线性插值保持平滑）\n",
    "    mask_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # 5折交叉验证\n",
    "    skf = StratifiedKFold(n_splits=config['k_folds'], shuffle=True, random_state=42)\n",
    "    X = annotations\n",
    "    y = annotations['LABEL']\n",
    "    \n",
    "    # 用于存储所有折的预测结果和热图\n",
    "    all_train_labels = []\n",
    "    all_train_preds = []\n",
    "    all_train_probs = []\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "    all_val_probs = []\n",
    "    \n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n{'='*20} Fold {fold + 1}/{config['k_folds']} {'='*20}\")\n",
    "        print(f\"使用宽松掩码约束模式，强度: {config['mask_strength']}\")\n",
    "        \n",
    "        # 准备数据\n",
    "        train_df = X.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = X.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        train_dataset = RenalFibrosisDataset(train_df, image_dir, mask_dir, train_transforms, mask_transforms)\n",
    "        val_dataset = RenalFibrosisDataset(val_df, image_dir, mask_dir, val_transforms, mask_transforms)\n",
    "        \n",
    "        # 初始化数据加载器\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 初始化模型（使用宽松约束）\n",
    "        model = RenalFibrosisModel(\n",
    "            num_classes=2,\n",
    "            model_name=config['model_name'],\n",
    "            pretrained=True,\n",
    "            mask_constraint=config['mask_constraint']\n",
    "        ).to(device)\n",
    "        \n",
    "        # 损失函数\n",
    "        class_weights = train_dataset.class_weights.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # 优化器\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 修改：使用新的模型保存路径\n",
    "        early_stopping_path = os.path.join(config['model_save_dir'], f'best_model_fold{fold}_soft.pth')\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=config['patience'],\n",
    "            verbose=True,\n",
    "            path=early_stopping_path\n",
    "        )\n",
    "        \n",
    "        # 初始化最佳记录\n",
    "        best_acc = 0.0\n",
    "        best_auc = 0.0\n",
    "        # 修改：使用新的模型保存路径\n",
    "        best_acc_model_path = os.path.join(config['model_save_dir'], f'best_acc_model_fold{fold}_soft.pth')\n",
    "        best_auc_model_path = os.path.join(config['model_save_dir'], f'best_auc_model_fold{fold}_soft.pth')\n",
    "        \n",
    "        # 用于存储本折的最佳预测结果\n",
    "        best_train_labels = None\n",
    "        best_train_preds = None\n",
    "        best_train_probs = None\n",
    "        best_val_labels = None\n",
    "        best_val_preds = None\n",
    "        best_val_probs = None\n",
    "        \n",
    "        # 训练循环\n",
    "        for epoch in range(config['num_epochs']):\n",
    "            print(f'\\nEpoch {epoch + 1}/{config[\"num_epochs\"]}')\n",
    "            \n",
    "            # 训练\n",
    "            model.train()\n",
    "            train_loss, train_correct = 0.0, 0\n",
    "            total_train = 0\n",
    "            train_probs, train_labels = [], []\n",
    "            \n",
    "            for inputs, masks, labels in tqdm(train_loader, desc='Training'):\n",
    "                # 过滤无效样本\n",
    "                valid_mask = labels != -1\n",
    "                if not valid_mask.any():\n",
    "                    continue\n",
    "                    \n",
    "                inputs = inputs[valid_mask].to(device)\n",
    "                masks = masks[valid_mask].to(device)\n",
    "                labels = labels[valid_mask].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs, heatmaps, _ = model(inputs, masks)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "                train_correct += torch.sum(preds == labels.data)\n",
    "                total_train += inputs.size(0)\n",
    "                \n",
    "                # 收集训练集的预测概率和真实标签\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                train_probs.extend(probs.detach().cpu().numpy())\n",
    "                train_labels.extend(labels.detach().cpu().numpy())\n",
    "            \n",
    "            # 验证\n",
    "            model.eval()\n",
    "            val_loss, val_correct = 0.0, 0\n",
    "            total_val = 0\n",
    "            val_probs, val_labels = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, masks, labels in tqdm(val_loader, desc='Validation'):\n",
    "                    valid_mask = labels != -1\n",
    "                    if not valid_mask.any():\n",
    "                        continue\n",
    "                        \n",
    "                    inputs = inputs[valid_mask].to(device)\n",
    "                    masks = masks[valid_mask].to(device)\n",
    "                    labels = labels[valid_mask].to(device)\n",
    "                    \n",
    "                    outputs, heatmaps, _ = model(inputs, masks)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    val_correct += torch.sum(preds == labels.data)\n",
    "                    total_val += inputs.size(0)\n",
    "                    \n",
    "                    probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                    val_probs.extend(probs.cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                    # 在每个epoch的最后一批验证数据上可视化热图\n",
    "                    if epoch == config['num_epochs'] - 1:\n",
    "                        # 修改：热图保存路径也放在模型保存目录下\n",
    "                        heatmap_dir = os.path.join(config['model_save_dir'], 'heatmaps_soft')\n",
    "                        os.makedirs(heatmap_dir, exist_ok=True)\n",
    "                        for i in range(min(3, inputs.size(0))):  # 可视化前3个样本\n",
    "                            attention_map = model.generate_attention_map(inputs[i:i+1], masks[i:i+1])\n",
    "                            visualize_heatmap(\n",
    "                                inputs[i], masks[i], attention_map,\n",
    "                                os.path.join(heatmap_dir, f'fold{fold}_epoch{epoch}_sample{i}.png'),\n",
    "                                mask_strength=config['mask_strength']\n",
    "                            )\n",
    "            \n",
    "            # 计算训练集和验证集指标\n",
    "            train_loss = train_loss / total_train if total_train > 0 else 0\n",
    "            train_acc = train_correct.double() / total_train if total_train > 0 else 0\n",
    "            val_loss = val_loss / total_val if total_val > 0 else 0\n",
    "            val_acc = val_correct.double() / total_val if total_val > 0 else 0\n",
    "            \n",
    "            # 计算各项指标\n",
    "            train_metrics = calculate_metrics(\n",
    "                np.array(train_labels), \n",
    "                np.array(train_probs) > 0.5, \n",
    "                np.array(train_probs)\n",
    "            )\n",
    "            val_metrics = calculate_metrics(\n",
    "                np.array(val_labels), \n",
    "                np.array(val_probs) > 0.5, \n",
    "                np.array(val_probs)\n",
    "            )\n",
    "            \n",
    "            print(f'\\nTrain Metrics - Loss: {train_loss:.4f} | Acc: {train_acc:.4f}')\n",
    "            print(f\"Train - Accuracy: {train_metrics['accuracy']:.4f} | Precision: {train_metrics['precision']:.4f} | Recall: {train_metrics['recall']:.4f} | F1: {train_metrics['f1']:.4f} | AUC-ROC: {train_metrics['roc_auc']:.4f}\")\n",
    "            \n",
    "            print(f'\\nVal Metrics - Loss: {val_loss:.4f} | Acc: {val_acc:.4f}')\n",
    "            print(f\"Val - Accuracy: {val_metrics['accuracy']:.4f} | Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f} | F1: {val_metrics['f1']:.4f} | AUC-ROC: {val_metrics['roc_auc']:.4f}\")\n",
    "            \n",
    "            # 早停检查\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "            # 保存最佳ACC模型和预测结果\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), best_acc_model_path)\n",
    "                print(f\"New best ACC model saved with accuracy: {best_acc:.4f}\")\n",
    "                \n",
    "                # 保存最佳预测结果\n",
    "                best_train_labels = train_labels\n",
    "                best_train_preds = (np.array(train_probs) > 0.5).astype(int)\n",
    "                best_train_probs = train_probs\n",
    "                best_val_labels = val_labels\n",
    "                best_val_preds = (np.array(val_probs) > 0.5).astype(int)\n",
    "                best_val_probs = val_probs\n",
    "            \n",
    "            # 保存最佳AUC模型\n",
    "            if val_metrics['roc_auc'] > best_auc:\n",
    "                best_auc = val_metrics['roc_auc']\n",
    "                torch.save(model.state_dict(), best_auc_model_path)\n",
    "                print(f\"New best AUC model saved with AUC: {best_auc:.4f}\")\n",
    "        \n",
    "        # 保存本折的最佳预测结果\n",
    "        if best_train_labels is not None:\n",
    "            all_train_labels.extend(best_train_labels)\n",
    "            all_train_preds.extend(best_train_preds)\n",
    "            all_train_probs.extend(best_train_probs)\n",
    "            all_val_labels.extend(best_val_labels)\n",
    "            all_val_preds.extend(best_val_preds)\n",
    "            all_val_probs.extend(best_val_probs)\n",
    "            \n",
    "            # 保存本折指标\n",
    "            fold_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'best_acc': best_acc,\n",
    "                'best_auc': best_auc,\n",
    "                'train_metrics': calculate_metrics(best_train_labels, best_train_preds, best_train_probs),\n",
    "                'val_metrics': calculate_metrics(best_val_labels, best_val_preds, best_val_probs),\n",
    "                'best_epoch': epoch - early_stopping.counter\n",
    "            })\n",
    "    \n",
    "    # 保存和输出最终结果\n",
    "    if len(all_val_labels) > 0:\n",
    "        # 计算合并后的指标\n",
    "        combined_train_metrics = calculate_metrics(\n",
    "            np.array(all_train_labels),\n",
    "            np.array(all_train_preds),\n",
    "            np.array(all_train_probs)\n",
    "        )\n",
    "        \n",
    "        combined_val_metrics = calculate_metrics(\n",
    "            np.array(all_val_labels),\n",
    "            np.array(all_val_preds),\n",
    "            np.array(all_val_probs)\n",
    "        )\n",
    "        \n",
    "        # 打印结果\n",
    "        print(\"\\n\\n================ Final Combined Results (Soft Mask Constraint) ================\")\n",
    "        print(f\"Mask Constraint Strength: {config['mask_strength']}\")\n",
    "        print(\"\\nCombined Training Metrics:\")\n",
    "        print(f\"Accuracy:    {combined_train_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision:   {combined_train_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:      {combined_train_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-score:    {combined_train_metrics['f1']:.4f}\")\n",
    "        print(f\"ROC-AUC:     {combined_train_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"PR-AUC:      {combined_train_metrics['pr_auc']:.4f}\")\n",
    "        print(f\"Kappa:       {combined_train_metrics['kappa']:.4f}\")\n",
    "        \n",
    "        print(\"\\nCombined Validation Metrics:\")\n",
    "        print(f\"Accuracy:    {combined_val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision:   {combined_val_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:      {combined_val_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-score:    {combined_val_metrics['f1']:.4f}\")\n",
    "        print(f\"ROC-AUC:     {combined_val_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"PR-AUC:      {combined_val_metrics['pr_auc']:.4f}\")\n",
    "        print(f\"Kappa:       {combined_val_metrics['kappa']:.4f}\")\n",
    "        \n",
    "        # 修改：结果文件也保存到模型目录\n",
    "        fold_metrics_path = os.path.join(config['model_save_dir'], 'fold_metrics_soft_mask.csv')\n",
    "        fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "        fold_metrics_df.to_csv(fold_metrics_path, index=False)\n",
    "        \n",
    "        # 保存合并的预测结果\n",
    "        combined_results = {\n",
    "            'train': {\n",
    "                'labels': [int(x) for x in all_train_labels],\n",
    "                'preds': [int(x) for x in all_train_preds],\n",
    "                'probs': [float(x) for x in all_train_probs]\n",
    "            },\n",
    "            'val': {\n",
    "                'labels': [int(x) for x in all_val_labels],\n",
    "                'preds': [int(x) for x in all_val_preds],\n",
    "                'probs': [float(x) for x in all_val_probs]\n",
    "            },\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        combined_predictions_path = os.path.join(config['model_save_dir'], 'combined_predictions_soft_mask.json')\n",
    "        with open(combined_predictions_path, 'w') as f:\n",
    "            json.dump(combined_results, f, indent=4)\n",
    "        \n",
    "        results = {\n",
    "            'config': config,\n",
    "            'combined_train_metrics': convert_tensors_to_serializable(combined_train_metrics),\n",
    "            'combined_val_metrics': convert_tensors_to_serializable(combined_val_metrics),\n",
    "            'fold_metrics': convert_tensors_to_serializable(fold_metrics)\n",
    "        }\n",
    "\n",
    "        final_results_path = os.path.join(config['model_save_dir'], 'final_results_soft_mask.json')\n",
    "        with open(final_results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # 可视化合并ROC曲线\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        train_fpr, train_tpr, _ = roc_curve(all_train_labels, all_train_probs)\n",
    "        train_auc = combined_train_metrics['roc_auc']\n",
    "        plt.plot(train_fpr, train_tpr, color='blue', linestyle='--', \n",
    "                label=f'Train ROC (AUC = {train_auc:.3f})')\n",
    "        \n",
    "        val_fpr, val_tpr, _ = roc_curve(all_val_labels, all_val_probs)\n",
    "        val_auc = combined_val_metrics['roc_auc']\n",
    "        plt.plot(val_fpr, val_tpr, color='red', \n",
    "                label=f'Val ROC (AUC = {val_auc:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'Soft Mask Constraint ROC (Strength: {config[\"mask_strength\"]})')\n",
    "        plt.legend()\n",
    "        # 修改：ROC曲线也保存到模型目录\n",
    "        roc_curve_path = os.path.join(config['model_save_dir'], 'combined_roc_curve_soft_mask.png')\n",
    "        plt.savefig(roc_curve_path, dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"\\n训练完成！所有结果和模型已保存到: {config['model_save_dir']}\")\n",
    "    print(\"宽松约束的热图可视化已保存到 'heatmaps_soft' 目录\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc2ba3-aae4-4d3b-91ea-4cd50b50915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ensure_align_heatmap_with_original():\n",
    "    \"\"\"确保热图与原图完美对齐的完整版本\"\"\"\n",
    "    \n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model_path = \"F.pth\"\n",
    "    if not os.path.exists(model_path):\n",
    "        model_files = [f for f in os.listdir('.') if f.startswith('best_auc_model') and f.endswith('_soft.pth')]\n",
    "        if not model_files:\n",
    "            model_files = [f for f in os.listdir('.') if f.startswith('best_auc_model') and f.endswith('.pth')]\n",
    "        if model_files:\n",
    "            model_path = model_files[0]\n",
    "        else:\n",
    "            raise FileNotFoundError(\"未找到训练好的模型文件\")\n",
    "    \n",
    "    model = RenalFibrosisModel(\n",
    "        num_classes=2, \n",
    "        model_name='resnet18', \n",
    "        pretrained=False, \n",
    "        mask_constraint='soft'\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"模型加载成功: {model_path}\")\n",
    "    print(f\"使用宽松掩码约束模式\")\n",
    "    \n",
    "    # 指定路径\n",
    "    image_folder = os.path.join(data_dir, \"Fibrosis\")\n",
    "    mask_folder = os.path.join(data_dir, \"Mask\")\n",
    "    output_folder = os.path.join(data_dir, \"Fibrosisheatmap\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "    mask_strength = 0.6\n",
    "    \n",
    "    def correct_interpolate_heatmap(heatmap, target_size):\n",
    "        \"\"\"正确地上采样热图到目标尺寸\"\"\"\n",
    "        # heatmap 的维度应该是 [H, W]\n",
    "        if heatmap.dim() == 2:\n",
    "            # 添加 batch 和 channel 维度: [1, 1, H, W]\n",
    "            heatmap_4d = heatmap.unsqueeze(0).unsqueeze(0)\n",
    "        else:\n",
    "            # 如果已经是3维或4维，确保是 [1, 1, H, W] 格式\n",
    "            heatmap_4d = heatmap.unsqueeze(0) if heatmap.dim() == 3 else heatmap\n",
    "            if heatmap_4d.size(1) != 1:\n",
    "                heatmap_4d = heatmap_4d.unsqueeze(1)\n",
    "        \n",
    "        # 上采样到目标尺寸 (height, width)\n",
    "        heatmap_resized = F.interpolate(\n",
    "            heatmap_4d, \n",
    "            size=target_size,\n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # 移除添加的维度，返回 [H, W]\n",
    "        return heatmap_resized.squeeze()\n",
    "    \n",
    "    def create_perfect_alignment(original_img, original_mask, heatmap, mask_strength=0.6):\n",
    "        \"\"\"创建完美对齐的热图叠加\"\"\"\n",
    "        \n",
    "        # 转换为numpy数组\n",
    "        original_img_np = np.array(original_img)\n",
    "        original_mask_np = np.array(original_mask)\n",
    "        \n",
    "        # 确保图像是3通道\n",
    "        if original_img_np.ndim == 2:\n",
    "            original_img_np = np.stack([original_img_np] * 3, axis=-1)\n",
    "        elif original_img_np.shape[2] == 1:\n",
    "            original_img_np = np.concatenate([original_img_np] * 3, axis=-1)\n",
    "        \n",
    "        # 确保掩码是2维\n",
    "        if original_mask_np.ndim == 3:\n",
    "            original_mask_np = original_mask_np[:, :, 0]\n",
    "        original_mask_np = (original_mask_np > 128).astype(np.float32)\n",
    "        \n",
    "        # 获取原始尺寸\n",
    "        original_height, original_width = original_img_np.shape[:2]\n",
    "        \n",
    "        # 将热图转换为numpy（如果还是tensor）\n",
    "        if torch.is_tensor(heatmap):\n",
    "            heatmap_np = heatmap.cpu().numpy()\n",
    "        else:\n",
    "            heatmap_np = heatmap\n",
    "        \n",
    "        # 确保热图是2维\n",
    "        if heatmap_np.ndim > 2:\n",
    "            heatmap_np = heatmap_np.squeeze()\n",
    "        \n",
    "        # 使用OpenCV将热图调整到原始尺寸（最可靠的方法）\n",
    "        heatmap_resized = cv2.resize(heatmap_np, (original_width, original_height), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # 同样调整掩码尺寸（确保完全一致）\n",
    "        mask_resized = cv2.resize(original_mask_np, (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # 应用宽松约束\n",
    "        soft_mask = mask_resized * mask_strength + (1 - mask_strength)\n",
    "        constrained_heatmap = heatmap_resized * soft_mask\n",
    "        \n",
    "        # 归一化热图\n",
    "        if constrained_heatmap.max() > constrained_heatmap.min():\n",
    "            heatmap_normalized = (constrained_heatmap - constrained_heatmap.min()) / (constrained_heatmap.max() - constrained_heatmap.min() + 1e-8)\n",
    "        else:\n",
    "            heatmap_normalized = constrained_heatmap\n",
    "        \n",
    "        # 创建彩色热图\n",
    "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_normalized), cv2.COLORMAP_JET)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 完美对齐叠加\n",
    "        overlay = cv2.addWeighted(original_img_np, 0.6, heatmap_colored, 0.4, 0)\n",
    "        \n",
    "        return {\n",
    "            'original_img': original_img_np,\n",
    "            'mask': mask_resized,\n",
    "            'heatmap': heatmap_normalized,\n",
    "            'overlay': overlay,\n",
    "            'constrained_heatmap': constrained_heatmap,\n",
    "            'original_size': (original_width, original_height)\n",
    "        }\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    for image_file in os.listdir(image_folder):\n",
    "        file_path = os.path.join(image_folder, image_file)\n",
    "        file_ext = Path(image_file).suffix.lower()\n",
    "        \n",
    "        if file_ext in supported_formats and os.path.isfile(file_path):\n",
    "            try:\n",
    "                print(f\"\\n处理图像: {image_file}\")\n",
    "                file_name = Path(image_file).stem\n",
    "                \n",
    "                # 查找掩码文件\n",
    "                mask_file = None\n",
    "                mask_patterns = [\n",
    "                    f\"{file_name}_mask.png\", f\"{file_name}_mask.jpg\",\n",
    "                    f\"{file_name}.png\", f\"{file_name}.jpg\",\n",
    "                    f\"{file_name}_mask.tif\", f\"{file_name}.tif\"\n",
    "                ]\n",
    "                \n",
    "                for pattern in mask_patterns:\n",
    "                    mask_path = os.path.join(mask_folder, pattern)\n",
    "                    if os.path.exists(mask_path):\n",
    "                        mask_file = pattern\n",
    "                        break\n",
    "                \n",
    "                if mask_file is None:\n",
    "                    print(f\"警告: 未找到 {file_name} 对应的掩码文件，跳过处理\")\n",
    "                    continue\n",
    "                \n",
    "                # 读取原始图像和掩码\n",
    "                original_img_pil = Image.open(file_path).convert('RGB')\n",
    "                original_mask_pil = Image.open(os.path.join(mask_folder, mask_file)).convert('L')\n",
    "                \n",
    "                original_size = original_img_pil.size\n",
    "                print(f\"原始图像尺寸: {original_size}\")\n",
    "                \n",
    "                # 数据预处理\n",
    "                data_transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                mask_transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "                \n",
    "                # 准备模型输入\n",
    "                model_input_img = data_transform(original_img_pil).unsqueeze(0).to(device)\n",
    "                model_input_mask = mask_transform(original_mask_pil).unsqueeze(0).to(device)\n",
    "                model_input_mask = (model_input_mask > 0.5).float()\n",
    "                \n",
    "                # 生成热图\n",
    "                with torch.no_grad():\n",
    "                    outputs, heatmap, features = model(model_input_img, model_input_mask)\n",
    "                    attention_map = model.generate_attention_map(model_input_img, model_input_mask)\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    pred_class = torch.argmax(probs, dim=1).item()\n",
    "                    confidence = probs[0, pred_class].item()\n",
    "                \n",
    "                print(f\"生成的热图尺寸: {attention_map.shape}\")\n",
    "                \n",
    "                # 创建完美对齐的结果\n",
    "                alignment_result = create_perfect_alignment(\n",
    "                    original_img_pil, original_mask_pil, attention_map, mask_strength\n",
    "                )\n",
    "                \n",
    "                # 创建详细的可视化\n",
    "                fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "                \n",
    "                # 第一行：原始数据和模型输入\n",
    "                axes[0, 0].imshow(alignment_result['original_img'])\n",
    "                axes[0, 0].set_title(f\"原始图像\\n{original_size[0]}x{original_size[1]}\")\n",
    "                axes[0, 0].axis('off')\n",
    "                \n",
    "                axes[0, 1].imshow(alignment_result['mask'], cmap='gray')\n",
    "                axes[0, 1].set_title(\"对齐后的掩码\")\n",
    "                axes[0, 1].axis('off')\n",
    "                \n",
    "                # 显示模型输入尺寸的图像\n",
    "                model_img_np = model_input_img[0].cpu().numpy()\n",
    "                model_img_np = np.transpose(model_img_np, (1, 2, 0))\n",
    "                model_img_np = model_img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                model_img_np = np.clip(model_img_np, 0, 1)\n",
    "                \n",
    "                axes[0, 2].imshow(model_img_np)\n",
    "                axes[0, 2].set_title(\"模型输入图像\\n224x224\")\n",
    "                axes[0, 2].axis('off')\n",
    "                \n",
    "                # 显示原始热图（模型输出尺寸）\n",
    "                original_heatmap = attention_map.squeeze().cpu().numpy()\n",
    "                axes[0, 3].imshow(original_heatmap, cmap='jet')\n",
    "                axes[0, 3].set_title(\"原始热图输出\\n224x224\")\n",
    "                axes[0, 3].axis('off')\n",
    "                \n",
    "                # 第二行：对齐后的结果\n",
    "                axes[1, 0].imshow(alignment_result['heatmap'], cmap='jet')\n",
    "                heatmap_size = alignment_result['heatmap'].shape\n",
    "                axes[1, 0].set_title(f\"对齐后热图\\n{heatmap_size[1]}x{heatmap_size[0]}\")\n",
    "                axes[1, 0].axis('off')\n",
    "                \n",
    "                axes[1, 1].imshow(alignment_result['overlay'])\n",
    "                class_name = \"纤维化\" if pred_class == 1 else \"正常\"\n",
    "                axes[1, 1].set_title(f\"完美对齐叠加\\n预测: {class_name}\\n置信度: {confidence:.3f}\")\n",
    "                axes[1, 1].axis('off')\n",
    "                \n",
    "                # 热图强度分布\n",
    "                masked_heatmap = alignment_result['constrained_heatmap'][alignment_result['mask'] > 0.5]\n",
    "                background_heatmap = alignment_result['constrained_heatmap'][alignment_result['mask'] <= 0.5]\n",
    "                \n",
    "                if len(masked_heatmap) > 0:\n",
    "                    axes[1, 2].hist(masked_heatmap, bins=50, alpha=0.6, color='red', label='掩码区域')\n",
    "                    if len(background_heatmap) > 0:\n",
    "                        axes[1, 2].hist(background_heatmap, bins=50, alpha=0.5, color='blue', label='背景区域')\n",
    "                    axes[1, 2].set_title(\"热图强度分布\")\n",
    "                    axes[1, 2].set_xlabel(\"强度值\")\n",
    "                    axes[1, 2].set_ylabel(\"像素数量\")\n",
    "                    axes[1, 2].legend()\n",
    "                \n",
    "                # 详细统计信息\n",
    "                mask_coverage = np.sum(alignment_result['mask']) / alignment_result['mask'].size * 100\n",
    "                mask_mean_intensity = np.mean(masked_heatmap) if len(masked_heatmap) > 0 else 0\n",
    "                bg_mean_intensity = np.mean(background_heatmap) if len(background_heatmap) > 0 else 0\n",
    "                \n",
    "                info_text = f\"\"\"对齐验证信息:\n",
    "原始尺寸: {original_size[0]}x{original_size[1]}\n",
    "热图尺寸: {heatmap_size[1]}x{heatmap_size[0]}\n",
    "掩码覆盖率: {mask_coverage:.1f}%\n",
    "掩码区域强度: {mask_mean_intensity:.3f}\n",
    "背景区域强度: {bg_mean_intensity:.3f}\n",
    "约束强度: {mask_strength}\n",
    "预测结果: {class_name}\n",
    "置信度: {confidence:.3f}\"\"\"\n",
    "                \n",
    "                axes[1, 3].text(0.05, 0.95, info_text, fontsize=9, verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "                axes[1, 3].set_title(\"详细统计信息\")\n",
    "                axes[1, 3].axis('off')\n",
    "                \n",
    "                plt.suptitle(f'肾脏纤维化热图分析 - {file_name} (完美对齐)', fontsize=16, y=0.95)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # 保存综合结果\n",
    "                result_path = os.path.join(output_folder, f\"{file_name}_detailed_analysis.jpg\")\n",
    "                plt.savefig(result_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # 单独保存高分辨率叠加图像\n",
    "                overlay_highres = cv2.addWeighted(\n",
    "                    alignment_result['original_img'], 0.6, \n",
    "                    cv2.cvtColor(\n",
    "                        cv2.applyColorMap(\n",
    "                            np.uint8(255 * alignment_result['heatmap']), \n",
    "                            cv2.COLORMAP_JET\n",
    "                        ), \n",
    "                        cv2.COLOR_BGR2RGB\n",
    "                    ), 0.3, 0\n",
    "                )\n",
    "                \n",
    "                overlay_path = os.path.join(output_folder, f\"{file_name}_highres_overlay.jpg\")\n",
    "                cv2.imwrite(overlay_path, cv2.cvtColor(overlay_highres, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "                # 保存对齐检查图像（在四个角标记参考点）\n",
    "                check_img = alignment_result['original_img'].copy()\n",
    "                h, w = check_img.shape[:2]\n",
    "                # 标记四个角\n",
    "                cv2.circle(check_img, (10, 10), 8, (255, 255, 0), -1)  # 左上-黄色\n",
    "                cv2.circle(check_img, (w-10, 10), 8, (0, 255, 255), -1)  # 右上-青色\n",
    "                cv2.circle(check_img, (10, h-10), 8, (255, 0, 255), -1)  # 左下-粉色\n",
    "                cv2.circle(check_img, (w-10, h-10), 8, (0, 255, 0), -1)  # 右下-绿色\n",
    "                \n",
    "                check_overlay = cv2.addWeighted(check_img, 0.8, \n",
    "                                              cv2.cvtColor(\n",
    "                                                  cv2.applyColorMap(\n",
    "                                                      np.uint8(255 * alignment_result['heatmap']), \n",
    "                                                      cv2.COLORMAP_JET\n",
    "                                                  ), \n",
    "                                                  cv2.COLOR_BGR2RGB\n",
    "                                              ), 0.2, 0)\n",
    "                \n",
    "                cv2.imwrite(os.path.join(output_folder, f\"{file_name}_alignment_check.jpg\"), \n",
    "                           cv2.cvtColor(check_overlay, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "                # 新增：保存原始图像-热图-叠加效果三合一图像\n",
    "                fig_triple, axes_triple = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                \n",
    "                # 原始图像\n",
    "                axes_triple[0].imshow(alignment_result['original_img'])\n",
    "                axes_triple[0].set_title(f\"Original\\n{original_size[0]}x{original_size[1]}\")\n",
    "                axes_triple[0].axis('off')\n",
    "                \n",
    "                # 热图\n",
    "                im_heatmap = axes_triple[1].imshow(alignment_result['heatmap'], cmap='jet')\n",
    "                axes_triple[1].set_title(f\"Grad-CAM\\n{heatmap_size[1]}x{heatmap_size[0]}\")\n",
    "                axes_triple[1].axis('off')\n",
    "                # 添加颜色条\n",
    "                plt.colorbar(im_heatmap, ax=axes_triple[1], fraction=0.046, pad=0.04)\n",
    "                \n",
    "                # 叠加效果\n",
    "                axes_triple[2].imshow(alignment_result['overlay'])\n",
    "                axes_triple[2].set_title(f\"Overlay\\n预测: {class_name} (置信度: {confidence:.3f})\")\n",
    "                axes_triple[2].axis('off')\n",
    "                \n",
    "                plt.suptitle(f'肾脏纤维化分析 - {file_name}', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # 保存三合一图像\n",
    "                triple_path = os.path.join(output_folder, f\"{file_name}_triple_comparison.jpg\")\n",
    "                plt.savefig(triple_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"✓ 完美对齐完成: {file_name}\")\n",
    "                print(f\"  原始尺寸: {original_size[0]}x{original_size[1]}\")\n",
    "                print(f\"  热图尺寸: {alignment_result['heatmap'].shape[1]}x{alignment_result['heatmap'].shape[0]}\")\n",
    "                print(f\"  掩码覆盖率: {mask_coverage:.1f}%\")\n",
    "                print(f\"  预测: {class_name} (置信度: {confidence:.3f})\")\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ 处理 {image_file} 时出错: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"完美对齐处理完成! 成功处理 {processed_count} 个图像\")\n",
    "    print(f\"详细分析结果保存在: {output_folder}\")\n",
    "    print(f\"每个图像包含:\")\n",
    "    print(f\"  - 详细分析图 (_detailed_analysis.jpg)\")\n",
    "    print(f\"  - 高分辨率叠加图 (_highres_overlay.jpg)\")\n",
    "    print(f\"  - 对齐检查图 (_alignment_check.jpg)\")\n",
    "    print(f\"  - 三合一对比图 (_triple_comparison.jpg)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'RenalFibrosisModel' not in globals():\n",
    "        print(\"错误: 需要先定义 RenalFibrosisModel 类\")\n",
    "        print(\"请确保已经运行了包含模型定义的代码\")\n",
    "    else:\n",
    "        ensure_align_heatmap_with_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b67bf-8b6f-4dab-99ab-f5f12a9d08bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTHON 3.11",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
